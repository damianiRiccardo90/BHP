{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6xIoEgAutmD3CrK8yzlqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/BHP/blob/master/C4-Web_Hackery/Mapping_Web_Apps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *__Web Hackery__*\n",
        "\n",
        "The ability to analyze web applications is an absolutely critical skill for any attacker or penetration tester. In most modern networks, web applications present the largest attack surface and therefore are also the most common avenue for gaining access to the web applications themselves.\n",
        "\n",
        "You'll find a number of excellent web application tools written in Python, including __w3af__ and __sqlmap__. Quite frankly, topics such as _SQL injection_ have been beaten to death, and the tooling available is mature enough that we don't need to reinvent the wheel. Instead, we'll explore the basics of interacting with the web by using Python and then build on this knowledge to create reconnaissance and brute-force tooling. By creating a few different tools, you should learn the fundamental skills you need to build any type of web application assessment tool that your particular attack scenario calls for.\n",
        "\n",
        "In this chapter, we'll look at three scenarios for attacking a web app. In the first scenario, you know the web framework that the target uses, and that framework happens to be open source. A web app framework contains many files and directories within directories within directories. We'll create a map that shows the hierarchy of the web app locally and use that information to locate the real files and directories on the live target.\n",
        "\n",
        "In the second scenario, you know only the URL for your target, so we'll resort to brute-forcing the same kind of mapping by using a word list to generate a list of filepaths and directory names that may be present on the target. We'll then attempt to connect to the resulting list of possible paths against a live target.\n",
        "\n",
        "In the third scenario, you know the base URL of your target and its login page. We'll examine the login page and use a word list to brute-force a login."
      ],
      "metadata": {
        "id": "Nruv5Cc5tekK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__Using Web Libraries__*\n",
        "\n",
        "We'll start by going over the libraries you can use to interact with web services. When performing network-based attacks, you may be using your own machine or a machine inside the network you're attacking. If you are on a compromised machine, you'll have to make do with what you've got, which might be a bare-bones Python 2.x or Python 3.x installation. We'll take a look at what you can do in those situations using the standard library. For the remainder of the chapter, however, we'll assume you're on your attacker machine using the most up-to-date packages."
      ],
      "metadata": {
        "id": "fKgDLVYqtzzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__The urllib2 Library for Python 2.x__*\n",
        "\n",
        "You'll see the __urllib2__ library used in code written for Python 2.x. It's bundled into the standard library. Much like the __socket__ library for writing network tooling, people use the __urllib2__ library when creating tools to interact with web services. Let's take a look at code that makes a very simple _GET_ request to the No Starch Press website:"
      ],
      "metadata": {
        "id": "A2lTYub4HD6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib2\n",
        "\n",
        "url = \"https://www.nostarch.com\"\n",
        "# GET\n",
        "response = urllib2.urlopen(url) #[1]\n",
        "print(response.read()) #[2]\n",
        "response.close()"
      ],
      "metadata": {
        "id": "QMjF1Sh3HqCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the simplest example of how to make a _GET_ request to a website. We pass in a URL to the __urlopen__ function __[1]__, which returns a file-like object that allows us to read back the body of what the remote web server returns __[2]__. As we're just fetching the raw page from the No Starch website, no JavaScript or other client-side languages will execute.\n",
        "\n",
        "In most cases, however, you'll want more fine-grained control over how you make these requests, including being able to define specific headers, handle cookies, and create _POST_ requests. The __urllib2__ library includes a __Request__ class that gives you this level of control. The following example shows you how to create the same _GET_ request by using the __Request__ class and by defining a custom __User-Agent__ HTTP header:"
      ],
      "metadata": {
        "id": "pw8T30V3H4Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib2\n",
        "\n",
        "url = \"https://www.nostarch.com\"\n",
        "headers = {\"User-Agent\": \"Googlebot\"} #[1]\n",
        "\n",
        "request = urllib2.Request(url, headers=headers) #[2]\n",
        "response = urllib2.urlopen(request) #[3]\n",
        "\n",
        "print(response.read())\n",
        "response.close()"
      ],
      "metadata": {
        "id": "XXoKj4n0JA4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The construction of a __Request__ object is slightly different from our previous example. To create custom headers, we define a __headers__ dictionary __[1]__, which allows us to then set the header keys and values we want to use. In this case, we'll make our Python script appear to be the Googlebot. We then create our __Request__ object and pass in the __url__ and the __headers__ dictionary __[2]__, and then pass the __Request__ object to the __urlopen__ function call __[3]__. This returns a normal file-like object that we can use to read in the data from the remote website."
      ],
      "metadata": {
        "id": "wNCRLJgC-pI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__The urllib Library for Python 3.x__*\n",
        "\n",
        "In Python 3.x, the standard library provides the __urllib__ package, which splits the capabilities from the __urllib2__ package into the __urllib.request__ and __urllib.error__ subpackages. It also adds URL-parsing capability with the subpackage __urllib.parse__.\n",
        "\n",
        "To make an HTTP request with this package, you can code the request as a context manager using the __with__ statement. The resulting response should contain a byte string. Here's how to make a _GET_ request:"
      ],
      "metadata": {
        "id": "MEY1qrsk_aGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request #[1]\n",
        "\n",
        "url = \"http://boodelyboo.com\" #[2]\n",
        "# GET\n",
        "with urllib.request.urlopen(url) as response: #[3]\n",
        "    content = response.read() #[4]\n",
        "\n",
        "print(content)"
      ],
      "metadata": {
        "id": "kwzy2OOhAMb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import the packages we need __[1]__ and define the target URL __[2]__. Then, using the __urlopen__ method as a context manager, we make the request __[3]__ and read the response __[4]__.\n",
        "\n",
        "To create a _POST_ request, pass a data dictionary to the request object, encoded as bytes. This data dictionary should have the key-value pairs that the target web app expects. In this example, the __info__ dictionary contains the credentials (_user_, _passwd_) needed to log in to the target website:"
      ],
      "metadata": {
        "id": "Rn4m4bWDAqzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.parse\n",
        "import urllib.request\n",
        "\n",
        "info = {\"user\": \"tim\", \"passwd\": \"31337\"}\n",
        "# Data is now of type bytes\n",
        "data = urllib.parse.urlencode(info).encode() #[1]\n",
        "\n",
        "req = urllib.request.Request(url, data) #[2]\n",
        "# POST\n",
        "with urllib.request.urlopen(req) as response:\n",
        "    content = response.read() #[3]\n",
        "\n",
        "print(content)"
      ],
      "metadata": {
        "id": "bnXGXd2ZCUXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We encode the data dictionary that contains the login credentials to make it a bytes object __[1]__, put it into the _POST_ request __[2]__ that transmits the credentials, and receive the web app response to our login attempt __[3]__."
      ],
      "metadata": {
        "id": "xWI892FUDMgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__The request Library__*\n",
        "\n",
        "Even the official Python documentation recommends using the __requests__ library for a higher-level HTTP client interface. It's not in the standard library, so you have to install it. Here's how to do so using __pip__:\n",
        "```\n",
        "pip install requests\n",
        "```\n",
        "The __requests__ library is useful because it can automatically handle cookies for you, as you'll see in each example that follows, but especially in the example where we attack a WordPress site in \"Brute-Forcing HTML Form Authentication\" on Page 85. To make an HTTP request, do the following:"
      ],
      "metadata": {
        "id": "SLRG4vW0DcDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://boodelyboo.com\"\n",
        "# GET\n",
        "response = requests.get(url)\n",
        "\n",
        "data = {\"user\": \"tim\", \"passwd\": \"31337\"}\n",
        "# POST\n",
        "response = requests.post(url, data=data) #[1]\n",
        "# response.text = string; response.content = bytestring\n",
        "print(response.text) #[2]"
      ],
      "metadata": {
        "id": "uFxW25bBEYOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the __url__, the __request__, and a __data__ dictionary containing the __user__ and __passwd__ keys. Then we post that request __[1]__ and print the __text__ attribute (a string) __[2]__. If you would rather work with a byte string, use the __content__ attribute returned from the post. You'll see an example of that in \"Brute-Forcing HTML Form Authentication\" on page 85."
      ],
      "metadata": {
        "id": "gMgjxM5sFPuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__The lxml and BeautifulSoup Packages__*\n",
        "\n",
        "Once you have an HTTP response, either the __lxml__ or __BeautifulSoup__ package can help you parse the contents. Over the past few years, these two packages have become more similar. You can use the __lxml__ parser with the __BeautifulSoup__ package, and the __BeautifulSoup__ parser with the __lxml__ package.\n",
        "\n",
        "You'll see code from other hackers that use one or the other. The __lxml__ package provides a slightly faster parse, while the __BeautifulSoup__ package has logic to automatically detect the target HTML page's encoding. We will use the __lxml__ package here. Install either package with __pip__:\n",
        "```\n",
        "pip install lxml\n",
        "pip install beautifulsoup4\n",
        "```\n",
        "Suppose you have the HTML content from a request stored in a variable named __content__. Using __lxml__, you could retrieve the content and parse the links as follows:"
      ],
      "metadata": {
        "id": "HpODwefBFxOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO #[1]\n",
        "from lxml import etree\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://nostarch.com\"\n",
        "# GET\n",
        "r = requests.get(url) #[2]\n",
        "# content is of type \"bytes\"\n",
        "content = r.content\n",
        "\n",
        "parser = etree.HTMLParser()\n",
        "# Parse into tree\n",
        "content = etree.parse(BytesIO(content), parser=parser) #[3]\n",
        "# Find all \"a\" anchor elements.\n",
        "for link in content.findall(\"//a\"): #[4]\n",
        "    print(f\"{link.get(\"href\")} -> {link.text}\") #[5]"
      ],
      "metadata": {
        "id": "IfP_kdczHBJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the __ByteIO__ class from the __io__ module __[1]__ because we'll need it in order to use a byte string as a file object when we parse the HTTP response. Next, we perform the _GET_ request as usual __[2]__ and then use the __lxml__ HTML parser to parse the response. The parser expects a file-like object or a file-name. The __BytesIO__ class enables us to use the returned byte string content as a file-like object to pass to the __lxml__ parser __[3]__. We use a simple query to find all the __a__ (anchor) tags that contain links in the returned content __[4]__ and print the results. Each anchor tag defines a link. Its __href__ attribute specifies the URL of the link.\n",
        "\n",
        "Note the use of the f-string __[5]__ that actually does the writing. In Python 3.6 and later, you can use f-strings to create strings containing variable values enclosed inside braces. This allows you to easily do things like include the result of a function call (_link.get(\"href\")_) or a plain value (_link.text_) in your string.\n",
        "\n",
        "Using __BeautifulSoup__, you can do the same kind of parsing with this code. As you can see, the technique is very similar to our last example using __lxml__:"
      ],
      "metadata": {
        "id": "Wnv_4QDzOetu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "\n",
        "url = \"http://bing.com\"\n",
        "r = requests.get(url)\n",
        "# Parse into tree\n",
        "tree = bs(r.text, \"html.parser\") #[1]\n",
        "# Find all \"a\" anchor elements.\n",
        "for link in tree.find_all('a'): #[2]\n",
        "    print(f\"{link.get(\"href\")} -> {link.text}\") #[3]"
      ],
      "metadata": {
        "id": "jgWuT0lOkJ7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax is almost identical. We parse the content into a tree __[1]__, iterate over the links (__a__, or anchor, tags) __[2]__, and print the target (__href__ attribute) and the link text (__link.text__) __[3]__.\n",
        "\n",
        "If you're working from a compromised machine, you'll likely avoid installing these third-party packages to keep from making too much network noise, so you're stuck with whatever you have on hand, which may be a bare-bone Python 2 or Python 3 installation. That means you'll use the standard library (__urllib2__ or __urllib__, respectively).\n",
        "\n",
        "In the examples that follow, we assume you're on your attacking box, which means you can use the __requests__ package to contact web servers and __lxml__ to parse the output you retrieve.\n",
        "\n",
        "Now that you have the fundamental means to talk to web services and websites, let's create some useful tooling for any web application attack or penetration test."
      ],
      "metadata": {
        "id": "kik3NZgAkyTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *__Mapping Open Source Web App Installations__*\n",
        "\n",
        "Content management systems (__CMSs__) and blogging platforms such as Joomla, WordPress, and Drupal make starting a new blog or website simple, and they're relatively common in a shared hosting environment or even an enterprise network. All systems have their own challenges in terms of installation, configuration, and patch management, and these CMS suites are no exception. When an overworked sysadmin or a helpless web developer doesn't follow all security and installation procedures, it can be easy pickings for an attacker to gain access to the web server.\n",
        "\n",
        "Because we can download any open source web application and locally determine its file and directory structure, we can create a purpose-built scanner that can hunt for all files that are reachable on the remote target. This can root out leftover installation files, directories that should be protected by __.htaccess__ files, and other goodies that can assist an attacker in getting a toehold on the web server.\n",
        "\n",
        "This project also introduces you to using Python __Queue__ objects, which allow us to build a large, thread-safe stack of items and have multiple threads pick items for processing. This will enable our scanner to run very rapidly. Also, we can trust that we won't have race conditions since we're using a queue, which is thread-safe, rather than a list."
      ],
      "metadata": {
        "id": "am792f9otlR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__Mapping the WordPress Framework__*\n",
        "\n",
        "Suppose you know that your web app target uses the WordPress framework. Let's see what a WordPress installation looks like. Download and unzip a local copy of WordPress. You can get the latest version from _https://wordpress.org/download/_. Here, we're using version 5.4 of WordPress. Even though the file's layout may differ from the live server you're targeting, it provides us with a reasonable starting place for finding files and directories present in most versions. To get a map of the directories and filenames that come in a standard WordPress distribution, create a new file named __mapper.py__. Let's write a function called __gather_paths__ to walk down the distribution, inserting each full filepath into a queue called __web_paths__:"
      ],
      "metadata": {
        "id": "DQ1vWir0uAgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import os\n",
        "import queue\n",
        "import requests\n",
        "import sys\n",
        "import threading\n",
        "import time\n",
        "\n",
        "FILTERED = [\".jpg\", \".gif\", \".png\", \".css\"]\n",
        "TARGET = \"http://boodelyboo.com/wordpress\" #[1]\n",
        "THREADS = 10\n",
        "\n",
        "answers = queue.Queue()\n",
        "web_paths = queue.Queue() #[2]\n",
        "\n",
        "def gather_paths():\n",
        "    for root, _, files in os.walk('.'): #[3]\n",
        "        for fname in files:\n",
        "            if os.path.splittext(fname)[1] in FILTERED:\n",
        "                continue\n",
        "            path = os.path.join(root, fname)\n",
        "            if path.startswith('.'):\n",
        "                path = path[1:]\n",
        "            print(path)\n",
        "            web_paths.put(path)\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def chdir(path): #[4]\n",
        "    \"\"\"\n",
        "    On enter, change directory to specified path.\n",
        "    On exit, change directory back to original.\n",
        "    \"\"\"\n",
        "    this_dir = os.getcwd()\n",
        "    os.chdir(path)\n",
        "    try:\n",
        "        yield #[5]\n",
        "    finally:\n",
        "        os.chdir(this_dir) #[6]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with chdir(\"/home/rick/Downloads/wordpress\"): #[7]\n",
        "        gather_paths()\n",
        "    input(\"Press return to continue.\")"
      ],
      "metadata": {
        "id": "IjZpf0M3pzxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by defining the remote target website __[1]__ and creating a list of file extensions that we aren't interested in fingerprinting. This list can be different depending on the target application, but in this case we chose to omit images and style sheet files. Instead, we're targeting HTML, or text files, which are more likely to contain information useful for compromising the server. The __answers__ variable is the __Queue__ object where we'll put the file-paths we've located locally. The __web_paths__ variable __[2]__ is a second __Queue__ object where we'll store the files that we'll attempt to locate on the remote server.\n",
        "\n",
        "Within the __gather_paths__ function, we use the __os.walk__ function __[3]__ to walk through all of the files and directories in the local web application directory. As we walk through the files and directories, we build the full paths to the target files and test them against the list stored in __FILTERED__ to make sure we are looking for only the file types we want. For each valid file we find locally, we add it to the __web_paths__ variable's __Queue__.\n",
        "\n",
        "The __chdir__ context manager __[4]__ needs a bit of explanation. Context managers provide a cool programming pattern, especially if you're forgetful or just have too much to keep track of and want to simplify your life. You'll find them helpful when you've opened something and need to close it, locked something and need to release it, or changed something and need to reset it. You're probably familiar with built-in file managers like __open__ to open a file or __socket__ to use a socket.\n",
        "\n",
        "Generally, you create a context manager by creating a class with the __enter__ and __exit__ methods. The __enter__ method returns the resource that needs to be managed (like a file or socket), and the __exit__ method performs the cleanup operations (closing a file, for example).\n",
        "\n",
        "However, in situations where you don't need as much control, you can use the __@contextlib.contextmanager__ to create a simple context manager that converts a generator function into a context manager.\n",
        "\n",
        "This __chdir__ function enables you to execute code inside a different directory and guarantees that, when you exit, you'll be returned to the orginal directory. The __chdir__ generator function initializes the context by saving the original directory and changing into the new one, yields control back to __gather_paths__ __[5]__, and then reverts to the orginal directory __[6]__.\n",
        "\n",
        "Notice that the __chdir__ function definition contains __try__ and __finally__ blocks. You'll often encounter __try/except__ statements, but the __try/finally__ pair is less common. The __finally__ block always executes, regardless of any exceptions raised. We need this here because, no matter whether the directory change succeeds, we want the context to revert to the orginal directory. A toy example of the __try__ block shows what happens for each case:"
      ],
      "metadata": {
        "id": "CM32hj39r-S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    something_that_might_cause_an_error()\n",
        "except SomeError as e:\n",
        "    print(e)             # Show the error on the console\n",
        "    do_something_else()  # Take some alternative action\n",
        "else:\n",
        "    everything_is_fine() # This executes only if the try succeeded\n",
        "finally:\n",
        "    cleanup()            # This executes no matter what"
      ],
      "metadata": {
        "id": "3wAekB5JdYkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Returning to the mapping code, you can see in the __main__ block that you use the __chdir__ context manager inside a __with__ statement __[7]__, which calls the generator with the name of the directory in which to execute the code. In this example, we pass in the location where we unzipped the WordPress ZIP file. This location will be differrent on your machine, make sure you pass in your own location. Entering the __chdir__ function saves the current directory name and changes the working directory to the path specified as the argument to the function. It then yields control back to the main thread of execution, which is where the __gather_paths__ function is run. Once the __gather_paths__ function completes, we exit the context manager, the __finally__ clause executes, and the working directory is restored to the original location.\n",
        "\n",
        "You can, of course, use __os.chdir__ manually, but if you forget to undo the change, you'll find your program executing in an unexpected place. By using your new __chdir__ context manager, you know that you're automatically working in the right context and that, when you return, you're back to where you were before. You can keep this context manager function in your utilities and use it in your other scripts. Spending time writing clean, understandable utility functions like this pays dividends later, since you will use them over and over.\n",
        "\n",
        "Execute the program to walk down the WordPress distribution hierarchy and see the full paths printed to the console:\n",
        "```\n",
        "(bhp) rick@kali:~/bhp/bhp$ python mapper.py\n",
        "/license.txt\n",
        "/wp-settings.php\n",
        "/xmlrpc.php\n",
        "/wp-login.php\n",
        "/wp-blog-header.php\n",
        "/wp-config-sample.php\n",
        "/wp-mail.php\n",
        "/wp-signup.php\n",
        "--snip--\n",
        "/readme.html\n",
        "/wp-includes/class-requests.php\n",
        "/wp-includes/media.php\n",
        "/wp-includes/wlwmanifest.xml\n",
        "/wp-includes/ID3/readme.txt\n",
        "--snip--\n",
        "/wp-content/plugins/akismet/_inc/form.js\n",
        "/wp-content/plugins/akismet/_inc/akismet.js\n",
        "\n",
        "Press return to continue.\n",
        "```\n",
        "Now our __web_paths__ variable's __Queue__ is full of paths for checking. You can see that we've picked up some interesting results: Filepaths present in the local WordPress installation that we can test against a live target WordPress app, including _.txt_, _.js_, and _.xml_ files. Of course, you can build additional intelligence into the script to return only files you're interested in, such as files that contain the word _install_."
      ],
      "metadata": {
        "id": "lAzcXjlbd-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__Testing the Live Target__*\n",
        "\n",
        "Now that you have the paths to the WordPress files and directories, it's time to do something with them, namely, test your remote target to see which of the files found in your local filesystem are actually installed on the target. These are the files we can attack in a later phase, to brute-force a login or investigate for misconfigurations. Let's add the __test_remote__ function to the __mapper.py__ file:"
      ],
      "metadata": {
        "id": "aFnKGX-yhPPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_remote():\n",
        "    while not web_paths.empty(): #[1]\n",
        "        path = web_paths.get() #[2]\n",
        "        url = f\"{TARGET}{path}\"\n",
        "        # Your target may have throttling/lockout.\n",
        "        time.sleep(2) #[3]\n",
        "        r = requests.get(url)\n",
        "        if r.status_code == 200:\n",
        "            answers.put(url) #[4]\n",
        "            sys.stdout.write('+')\n",
        "        else:\n",
        "            sys.stdout('x')\n",
        "        sys.stdout.flush()"
      ],
      "metadata": {
        "id": "YBv-mUQohtYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The __test_remote__ function is the workhorse of the mapper. It operates in a loop that will keep executing until the __web_paths__ variable's __Queue__ is empty __[1]__. On each iteration of the loop, we grab a path from the __Queue__ __[2]__, add it to the target website's base path, and then attempt to retrieve it. If we get a success (indicated by the response code 200), we put that URL into the __answers__ queue __[4]__ and write a _+_ on the console. Otherwise, we write an _x_ on the console and continue the loop.\n",
        "\n",
        "Some web servers lock you out if you bombard them with requests. That's why we use a __time.sleep__ of two seconds __[3]__ to wait between each request, which hopefully slows the rate of our requests enough to bypass a lockout rule.\n",
        "\n",
        "Once you know how a target responds, you can remove the lines that write to the console, but when you're first touching the target, writing those _+_ and _x_ characters on the console helps you understand what's going on as you run your test.\n",
        "\n",
        "Finally, we write the __run__ function as the entry point to the mapper application:"
      ],
      "metadata": {
        "id": "byoFC2TuibDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    mythreads = list()\n",
        "    for i in range(THREADS): #[1]\n",
        "        print(f\"Spawning thread {i}\")\n",
        "        t = threading.Thread(target=test_remote) #[2]\n",
        "        mythreads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    for thread in mythreads:\n",
        "        thread.join() #[3]"
      ],
      "metadata": {
        "id": "kVlqHF3gj3YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The __run__ function orchestrates the mapping process, calling the functions just defined. We start 10 threads (defined at the beginning of the script) __[1]__ and have each thread run the __test_remote__ function __[2]__. We then wait for all 10 threads to complete (using __thread.join__) before returning __[3]__.\n",
        "\n",
        "Now, we can finish up by adding some more logic to the __\\_\\_main\\_\\___ block. Replace the file's original __\\_\\_main\\_\\___ block with this updated code:"
      ],
      "metadata": {
        "id": "W_G6tuIKkXAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    with chdir(\"/home/rick/Downloads/wordpress\"): #[1]\n",
        "        gather_paths()\n",
        "    input(\"Press return to continue.\") #[2]\n",
        "\n",
        "    run() #[3]\n",
        "    with open(\"myanswers.txt\", 'w') as f: #[4]\n",
        "        while not answers.empty():\n",
        "            f.write(f\"{answers.get()}\\n\")\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "id": "xWx-H_Q8k7X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the context manager __chdir__ __[1]__ to navigate to the right directory before we call __gather_paths__. We've added a paus there in case we want to review th console output before continuing __[2]__. At this point, we have gathered the interesting filepaths from our local installation. Then we run the main mapping task __[3]__ against the remote application and write the answers to a file. We'll likely get a bunch of successful requests, and when we print the successful URLs to the console, the results may go by so fast that we won't be able to follow. To avoid that, add a block __[4]__ to write the results to a file. Notice the context manager method to open a file. This guarantees that the file closes when the block is finished."
      ],
      "metadata": {
        "id": "2IA1mjYelc-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *__Kicking the Tires__*\n",
        "\n",
        "The authors keep a site around just for testing (_boodelyboo.com/_), and that's what we've targeted in this example. For you own tests, you might create a site to play with, or you can install WordPress into your Kali VM. Note that you use any open source web application that's quick to deploy or that you have running already. When you run __mapper.py__, you should see output like this:\n",
        "```\n",
        "Spawning thread 0\n",
        "Spawning thread 1\n",
        "Spawning thread 2\n",
        "Spawning thread 3\n",
        "Spawning thread 4\n",
        "Spawning thread 5\n",
        "Spawning thread 6\n",
        "Spawning thread 7\n",
        "Spawning thread 8\n",
        "Spawning thread 9\n",
        "++x+x+++x+x+x++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "```\n",
        "When the process is finished, the paths on which you were successful are listed in the new file __myanswers.txt__."
      ],
      "metadata": {
        "id": "5z8XDADWmUXr"
      }
    }
  ]
}