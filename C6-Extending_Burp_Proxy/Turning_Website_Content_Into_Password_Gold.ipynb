{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0nZnTbBwvFGj0qjzklRNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/BHP/blob/master/C6-Extending_Burp_Proxy/Turning_Website_Content_Into_Password_Gold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *__Turning Website Content into Password Gold__*\n",
        "\n",
        "Many times, security comes down to one thing: User passwords. It's sad but true. Making things worse, when it comes to web applications, especially custom ones, it's all too common to discover that they don't lock users out of their accounts after a certain number of failed authentication attempts. In other instances, they don't enforce strong passwords. In these cases, an online password guessing session like the one in the last chapter might be just the ticket to gain access to the site.\n",
        "\n",
        "The trick to online password guessing is getting the right word list. You can't test 10 million passwords if you're in a hurry, so you need to be able to create a word list targeted to the site in question. Of course, there are scripts in Kali Linux that crawl a website and generate a word list based on site content. But if you've already used Burp to scan the site, why send more traffic just to generate a word list? Plus, those scripts usually have a ton of command line arguments to remember. If you're anything like us, you've already memorized enough command line arguments to impress your friends, so let's make Burp do the heavy lifting.\n",
        "\n",
        "Open __bhp_wordlist.py__ and knock out this code:"
      ],
      "metadata": {
        "id": "GMXjQPTIfW7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from burp import IBurpExtender\n",
        "from burp import IContextMenuFactory\n",
        "\n",
        "from java.util import ArrayList\n",
        "from java.swing import JMenuItem\n",
        "\n",
        "from datetime import datetime\n",
        "from HTMLParser import HTMLParser\n",
        "\n",
        "import re\n",
        "\n",
        "class TagStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        self.page_text = []\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        self.page_text.append(data) #[1]\n",
        "\n",
        "    def handle_comment(self, data):\n",
        "        self.page_text.append(data) #[2]\n",
        "\n",
        "    def strip(self, html):\n",
        "        self.feed(html)\n",
        "        return \" \".join(self.page_text) #[3]\n",
        "\n",
        "class BurpExtender(IBurpExtender, IContextMenuFactory):\n",
        "    def registerExtenderCallbacks(self, callbacks):\n",
        "        self._callbacks = callbacks\n",
        "        self._helpers = callbacks.getHelpers()\n",
        "        self.context = None\n",
        "        self.hosts = set()\n",
        "\n",
        "        # Start with something we know is common\n",
        "        self.wordlist = set([\"password\"]) #[4]\n",
        "\n",
        "        # We set up our extension\n",
        "        callbacks.setExtensionName(\"BHP Wordlist\")\n",
        "        callbacks.registerContextMenuFactory(self)\n",
        "\n",
        "        return\n",
        "\n",
        "    def createMenuItems(self, context_menu):\n",
        "        self.context = context_menu\n",
        "        menu_list = ArrayList()\n",
        "        menu_list.add(JMenuItem(\"Create Wordlist\", actionPerformed=self.wordlist_menu))\n",
        "\n",
        "        return menu_list"
      ],
      "metadata": {
        "id": "W2zfPNc9g-zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this listing should be pretty familiar by now. We start by importing the required modules. A helper __TagStripper__ class will allow us to strip the HTML tags out of the HTTP responses we process later on. Its __handle_data__ method store the page text __[1]__ in a member variable. We also define the __handle_comment__ method because we want to add the words stored in developer comments to the password list as well. Under the covers, __handle_comment__ just calls __handle_data__ __[2]__ (in case we want to change how we process page text down the road).\n",
        "\n",
        "The __strip__ method feeds HTML code to the base class, __HTMLParser__, and returns the resulting page text __[3]__, which will come in handy later. The rest is almost exactly the same as the start of the __bhp_bing.py__ script we just finished. Once again, the goal is to create a context menu item in the Burp UI. The only thing new here is that we store our word list in a __set__, which ensures that we don't introduce duplicate words as we go. We initialize the __set__ with everyone's favorite password, __password__ __[4]__, just to make sure it ends up in our final list.\n",
        "\n",
        "Now let's add the logic to take the selected HTTP traffic from Burp and turn it into a base word list:"
      ],
      "metadata": {
        "id": "wgwMfOVDiccE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BurpExtender(IBurpExtender, IContextMenuFactory):\n",
        "    def wordlist_menu(self, event):\n",
        "         # Grab the details of what the user clicked\n",
        "         http_traffic = self.context.getSelectedMessages()\n",
        "\n",
        "         for traffic in http_traffic:\n",
        "             http_service = traffic.getHttpService()\n",
        "             host = http_service.getHost()\n",
        "             self.hosts.add(host) #[1]\n",
        "\n",
        "             http_response = traffic.getResponse()\n",
        "             if http_response:\n",
        "                 self.get_words(http_response) #[2]\n",
        "\n",
        "        self.display_wordlist()\n",
        "        return\n",
        "\n",
        "    def get_words(self, http_response):\n",
        "        headers, body = http_response.tostring().split(\"\\r\\n\\r\\n\", 1)\n",
        "\n",
        "        # Skip non-text responses\n",
        "        if headers.lower().find(\"content-type: text\") == -1: #[3]\n",
        "            return\n",
        "\n",
        "        tag_stripper = TagStripper()\n",
        "        page_text = tag-stripper.strip(body) #[4]\n",
        "\n",
        "        words = re.findall(\"[a-zA-Z]\\w{2,}\", page_text) #[5]\n",
        "\n",
        "        for word in words:\n",
        "            # Filter out long strings\n",
        "            if len(word) <= 12:\n",
        "                self.wordlist.add(word.lower()) #[6]\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "8a8j9guoChuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first order of business is to define the __wordlist_menu__ method, which handles menu clicks. It saves the name of the responding host __[1]__ for later and then retrieves the HTTP response and feeds it to the __get_words__ method __[2]__.\n",
        "From there, __get_words__ checks the response header to make sure we're processing text-based responses only __[3]__. The __TagStripper__ class __[4]__ strips the HTML code from the rest of the page text. We use a regular expression to find all words starting with an alphabetic character and two or more \"word\" characters as specified with the _\\w{2,}_ regular expression __[5]__. We save the words that match this pattern to the __wordlist__ in lowrcase __[6]__.\n",
        "\n",
        "Now let's polish the script by giving it the ability to mangle and display the capture word list:"
      ],
      "metadata": {
        "id": "tQKqYVyHEcQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BurpExtender(IBurpExtender, IContextMenuFactory):\n",
        "    def mangle(self, word):\n",
        "        year = datetime.now().year\n",
        "        suffixes = ['', '1', '!', year] #[1]\n",
        "        mangled = []\n",
        "\n",
        "        for password in (word, word.capitalize()):\n",
        "            for suffix in suffixes:\n",
        "                mangled.append(\"%s%s\" % (password, suffix)) #[2]\n",
        "\n",
        "        return mangled\n",
        "\n",
        "    def display_wordlist(self):\n",
        "        print(\"#!comment: BHP Wordlist for site(s) %s\" % \", \".join(self.hosts)) #[3]\n",
        "\n",
        "        for word in sorted(self.wordlist):\n",
        "            for password in self.mangle(word):\n",
        "                print(password)\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "qnewJdnaFtQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very nice! The __mangle__ method takes a base word and turns it into a number of password guesses based on some common password creation strategies. In this simple example, we create a list of suffixes to tack on the end of the base word, including the current year __[1]__. Next, we loop through each suffix and add it to the base word __[2]__ to create a unique password attempt. We do another loop with a capitalized version of the base word for good measure. In the __display_wordlist__ method, we print a \"John The Ripper\" style comment __[3]__ to remind us which sites we used to generate this word list. Then we mangle each base word and print the results. Time to take this baby for a spin."
      ],
      "metadata": {
        "id": "revVvPBUGhT0"
      }
    }
  ]
}